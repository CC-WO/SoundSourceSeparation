# SUMMARY

## 第一章 音源分離とは

音源分離のプログラムの構成

- 入力：混合音
- 処理：音源分離
- 出力：きれいな音

音源の分離の目的は, マイクロホンで捉える音の中に混ざって隠れている各音源の音を分離すること.

マイクロホンの観測音は以下の式で表せる.

<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+x_t+%3D+c_1+%2B+c_2%0A" 
alt="x_t = c_1 + c_2
">

x: 観測音(時間情報をもつ), c1: 音源1, c2: 音源2

通常, cはたくさんの音源であることが一般的で, 音源分離では一つのマイクロホンの信号から複数の音源を推定することとなる. 推定の手がかりとしてモデルを使用する.

音源分離では2つのモデルがある.

1. 空間モデル
1. 音源モデル

空間モデルでは, 音の伝播の仕方が場所ごとに異なることを利用して, 音の大小関係・時間差から音源を推定する. これを先ほどの数式を使って表現すると次のようになる.

<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+x_t+%3D+a_%7Bm%2C1%7Ds_1+%2B+a_%7Bm%2C2%7Ds_2%0A" 
alt="x_t = a_{m,1}s_1 + a_{m,2}s_2
">

am,1やam,2は音の減衰もしくは音の遅れを表す係数で, これは音源ごとに決まる.

もし, am,1とam,2がわかっているとすると, 未知の変数は2つということになります. 観測信号が2つあれば, 次の連立方程式を解くことで未知の2変数を求めることができる

<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+x_1+%3D+a_%7B1%2C1%7Ds_1+%2B+a_%7B1%2C2%7Ds_2" 
alt="x_1 = a_{1,1}s_1 + a_{1,2}s_2">

<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+x_2+%3D+a_%7B2%2C1%7Ds_1+%2B+a_%7B2%2C2%7Ds_2%0A" 
alt="x_2 = a_{2,1}s_1 + a_{2,2}s_2
">

したがって,

<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+s_1+%3D+%5Cfrac%7Bx_1a_%7B2%2C2%7D-x_2a_%7B1%2C2%7D%7D%7Ba_%7B1%2C1%7Da_%7B2%2C2%7D-a_%7B2%2C1%7Da_%7B1%2C2%7D%7D%0A" 
alt="s_1 = \frac{x_1a_{2,2}-x_2a_{1,2}}{a_{1,1}a_{2,2}-a_{2,1}a_{1,2}}
">

<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+s_2+%3D+%5Cfrac%7Bx_1a_%7B2%2C1%7D-x_2a_%7B1%2C1%7D%7D%7Ba_%7B1%2C2%7Da_%7B2%2C1%7D-a_%7B1%2C1%7Da_%7B2%2C2%7D%7D%0A" 
alt="s_2 = \frac{x_1a_{2,1}-x_2a_{1,1}}{a_{1,2}a_{2,1}-a_{1,1}a_{2,2}}
">

となる.

しかし, 実際には音源の場所はわからない場合も多いと考えられる. 入力信号しか手に入らない場合はam,1やam,2が事前に分かっているという仮定は強すぎる.

ここで仮にam,1やam,2がわかっていいなくても音を分離できる技術がある. それがブラインド音源分離と呼ばれる技術だ. その際にもう一つのモデルである音源モデルを用いる.

音源モデルを使う場合は, am,1やam,2をとりあえずランダムに設定し, 何かしらのs1,s2を推定します. この推定したs1,s2が音声らしいかどうかを評価できたとする. 逆に推定されたs1, s2が音声らしくなければam,1やam,2が正しくないということになるのでもう一度試す必要があるということになる. これを繰り返していきs1, s2が音声らしいものとなればよいというわけだ.

## 第二章 音声処理の基礎

### 波形とは

- 周波数
- 振幅
- 位相

によって決まる.

### サンプリング定理とは

`サンプリング周波数の半分より上の周波数成分については, サンプリングされた音声データから元々の連続的な波形を正しく復元することができない`

### フーリエ変換

ここで時系列データのことを時間領域のデータといい, 周波数成分の振幅と位相のデータを周波数領域のデータという.

`フーリエ変換は時間領域のデータを周波数領域のデータに変換する`

### 短時間フーリエ変換とは

`元となる音声データを短時間の時系列データに区切り, そのデータごとにフーリエ変換を施すこと`

短時間の時系列データの一つひとつをフレームと呼び, フーリエ変換した各フレームを時間方向に並べることで, 周波数ごとの振幅の変換を捉えることができる. 通常, 数十ミリ秒単位のデータごと(フレーム)にフーリエ変換する. またフレームを時間方向に少しずつオーバーラップさせることでより時間方向の周波数ごとの変化をより連続的に問えることができる.

### 窓間数とは

短時間フーリエ変換を行うときに, 短時間の時間領域に音声波形をフレームごとに切り出すが, その際に窓間数という関数をかけて波形を切り出す. これにより周波数に変換した祭に, 他の周波数が漏れ込む影響を軽減できる.

ハニング窓

<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+w%28n%29+%3D+0.5+-+0.5cos+%5Cfrac%7B2+%5Cpi+n%7D%7BN-1%7D%0A" 
alt="w(n) = 0.5 - 0.5cos \frac{2 \pi n}{N-1}
">

0 <= n <= N-1とする. Nをフレームサイズと呼び, フレーム内のサンプル数とする. nは切り出すフレームの先頭から数えたサンプル数となる.

ハミング窓

<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+w%28n%29+%3D+0.54+-+0.46+cos+%5Cfrac%7B2+%5Cpi+n%7D%7BN-1%7D%0A" 
alt="w(n) = 0.54 - 0.46 cos \frac{2 \pi n}{N-1}
">

同様に0 <= n <= N-1とする.

方形窓

単にある区間を区切ったもの.

<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+w%28n%29+%3D+1%0A" 
alt="w(n) = 1
">
